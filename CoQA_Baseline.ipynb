{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CoQA_Baseline.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1OulevNvDn8H8klAyZtFdAmvov1MqsMzr",
      "authorship_tag": "ABX9TyPEuiurQHcjdMHVDy4eGFmv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rezaafra/question-answering/blob/master/CoQA_Baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEaTKoj2O9D8"
      },
      "source": [
        "import torch\n",
        "from torch.jit import script, trace\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import json\n",
        "import csv\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import codecs\n",
        "from io import open\n",
        "import itertools\n",
        "import math\n",
        "import urllib.request\n",
        "\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-cKHyGNQ4zb"
      },
      "source": [
        "\n",
        "def _get_filename_from_url(url):\n",
        "    parse = urlparse(url)\n",
        "    return os.path.basename(parse.path)\n",
        "\n",
        "def download_file(url, directory, filename=None, extension=None):\n",
        "    if filename is None:\n",
        "        filename = _get_filename_from_url(url)\n",
        "\n",
        "    directory = str(directory)\n",
        "    filepath = os.path.join(directory, filename)\n",
        "\n",
        "    if not os.path.isdir(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    # Download\n",
        "    urllib.request.urlretrieve(url, filename=filepath)\n",
        "\n",
        "    return filepath"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z2hB14afMWm"
      },
      "source": [
        " def coqa_dataset(directory='data/',\n",
        "                  train=False,\n",
        "                  dev=False,\n",
        "                  train_filename='coqa-train-v1.0.json',\n",
        "                  dev_filename='coqa-dev-v1.0.json',\n",
        "                  url_train='https://nlp.stanford.edu/data/coqa/coqa-train-v1.0.json',\n",
        "                  url_dev='https://nlp.stanford.edu/data/coqa/coqa-dev-v1.0.json'):\n",
        "    \n",
        "    download_file(url=url_dev, directory=directory)\n",
        "    download_file(url=url_train, directory=directory)\n",
        "\n",
        "    ret = []\n",
        "    splits = [(train, train_filename), (dev, dev_filename)]\n",
        "    splits = [f for (requested, f) in splits if requested]\n",
        "    for filename in splits:\n",
        "        full_path = os.path.join(directory, filename)\n",
        "        with open(full_path, 'r') as temp:\n",
        "            ret.append(json.load(temp)['data'])\n",
        "\n",
        "    if len(ret) == 1:\n",
        "        return ret[0]\n",
        "    else:\n",
        "        return tuple(ret)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEt6mQ6RwrMV"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MvSkNG9rSNq"
      },
      "source": [
        "train, dev = coqa_dataset(train=True), coqa_dataset(dev=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2bX8gpTweDn",
        "outputId": "f301b1b9-7d6a-4c82-a835-ca40e0ba80b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "train[0][\"questions\"]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'input_text': 'When was the Vat formally opened?', 'turn_id': 1},\n",
              " {'input_text': 'what is the library for?', 'turn_id': 2},\n",
              " {'input_text': 'for what subjects?', 'turn_id': 3},\n",
              " {'input_text': 'and?', 'turn_id': 4},\n",
              " {'input_text': 'what was started in 2014?', 'turn_id': 5},\n",
              " {'input_text': 'how do scholars divide the library?', 'turn_id': 6},\n",
              " {'input_text': 'how many?', 'turn_id': 7},\n",
              " {'input_text': 'what is the official name of the Vat?', 'turn_id': 8},\n",
              " {'input_text': 'where is it?', 'turn_id': 9},\n",
              " {'input_text': 'how many printed books does it contain?', 'turn_id': 10},\n",
              " {'input_text': 'when were the Secret Archives moved from the rest of the library?',\n",
              "  'turn_id': 11},\n",
              " {'input_text': 'how many items are in this secret collection?',\n",
              "  'turn_id': 12},\n",
              " {'input_text': 'Can anyone use this library?', 'turn_id': 13},\n",
              " {'input_text': 'what must be requested to view?', 'turn_id': 14},\n",
              " {'input_text': 'what must be requested in person or by mail?', 'turn_id': 15},\n",
              " {'input_text': 'of what books?', 'turn_id': 16},\n",
              " {'input_text': 'What is the Vat the library of?', 'turn_id': 17},\n",
              " {'input_text': 'How many books survived the Pre Lateran period?',\n",
              "  'turn_id': 18},\n",
              " {'input_text': 'what is the point of the project started in 2014?',\n",
              "  'turn_id': 19},\n",
              " {'input_text': 'what will this allow?', 'turn_id': 20}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRwq-BMlwi0T"
      },
      "source": [
        "def extractSentencePairs(data):\n",
        "  qa_pairs = []\n",
        "  for each in data:\n",
        "    questions = each[\"questions\"] # a list of dictionaries of questions. exp: {'input_text': 'When was the Vat formally opened?', 'turn_id': 1} \n",
        "    answers = each[\"answers\"]\n",
        "    for q, a in zip(questions, answers):\n",
        "      qa_pairs.append([q[\"input_text\"], a[\"input_text\"]])\n",
        "  return qa_pairs"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOjRAZ6DSdoM"
      },
      "source": [
        "pairs = extracSentencePairs(train)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTvD6hBcShG5",
        "outputId": "75309085-6d69-45a5-ebe8-8045c587248b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Define path to new file\n",
        "datafile = os.path.join(\"data\", \"formatted_corpus.txt\")\n",
        "\n",
        "delimiter = '\\t'\n",
        "# Unescape the delimiter\n",
        "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
        "\n",
        "# Write new csv file\n",
        "print(\"\\nWriting newly formatted file...\")\n",
        "with open(datafile, 'w', encoding='utf-8') as outputfile:\n",
        "    writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n')\n",
        "    for pair in extractSentencePairs(train):\n",
        "        writer.writerow(pair)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Writing newly formatted file...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwzkH4icWZhZ"
      },
      "source": [
        "# Default word tokens\n",
        "PAD_token = 0  # Used for padding short sentences\n",
        "SOS_token = 1  # Start-of-sentence token\n",
        "EOS_token = 2  # End-of-sentence token\n",
        "\n",
        "class Voc:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.trimmed = False\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3  # Count SOS, EOS, PAD\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    # Remove words below a certain count threshold\n",
        "    def trim(self, min_count):\n",
        "        if self.trimmed:\n",
        "            return\n",
        "        self.trimmed = True\n",
        "\n",
        "        keep_words = []\n",
        "\n",
        "        for k, v in self.word2count.items():\n",
        "            if v >= min_count:\n",
        "                keep_words.append(k)\n",
        "\n",
        "        print('keep_words {} / {} = {:.4f}'.format(\n",
        "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
        "        ))\n",
        "\n",
        "        # Reinitialize dictionaries\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3 # Count default tokens\n",
        "\n",
        "        for word in keep_words:\n",
        "            self.addWord(word)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mI0XHEhbXZbn"
      },
      "source": [
        "# Default word tokens\n",
        "PAD_token = 0  # Used for padding short sentences\n",
        "SOS_token = 1  # Start-of-sentence token\n",
        "EOS_token = 2  # End-of-sentence token\n",
        "\n",
        "class Voc:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.trimmed = False\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3  # Count SOS, EOS, PAD\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    # Remove words below a certain count threshold\n",
        "    def trim(self, min_count):\n",
        "        if self.trimmed:\n",
        "            return\n",
        "        self.trimmed = True\n",
        "\n",
        "        keep_words = []\n",
        "\n",
        "        for k, v in self.word2count.items():\n",
        "            if v >= min_count:\n",
        "                keep_words.append(k)\n",
        "\n",
        "        print('keep_words {} / {} = {:.4f}'.format(\n",
        "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
        "        ))\n",
        "\n",
        "        # Reinitialize dictionaries\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3 # Count default tokens\n",
        "\n",
        "        for word in keep_words:\n",
        "            self.addWord(word)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPpfYqz2Z8j8"
      },
      "source": [
        "voc = Voc(\"formatted_corpus.txt\")\n",
        "for pair in pairs:\n",
        "    voc.addSentence(pair[0])\n",
        "    voc.addSentence(pair[1])"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHw0C8Pfd5Kw",
        "outputId": "a2ec46d4-7186-43b4-ea4e-4252c05e0472",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(voc.num_words)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "71875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thccN8lvd9Sx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}